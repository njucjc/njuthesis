#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass ../build/thesis
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8-plain
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format pdf4
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Assignment 3: 自然语言通顺与否的判定 
\end_layout

\begin_layout Author
姓名：陈锦赐
\begin_inset space \hspace{}
\length 0.5cm
\end_inset

学号：MF1833004
\end_layout

\begin_layout Date
2019年1月14日
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chapter
问题分析
\end_layout

\begin_layout Section
任务描述
\end_layout

\begin_layout Standard
本次实验的任务为然语言通顺与否的判定，具体来说就是给定一个句子，要求判定所给的句子是否通顺，若句子通顺输出0，反之输出1。
\end_layout

\begin_layout Section
语言模型
\end_layout

\begin_layout Standard
对于很多自然语言处理领域的问题，比如机器翻译，处理要确定预测结果中的字词集合以外，还有一个非常重要的方面就是要评估文本序列是否符合人类使用的习惯。也就是要判断文
本是否通顺、自然、甚至在翻译问题上，“信”、“达”、“雅”是一种高级的要求。语言模型就是用于评估文本符合语言使用习惯程度的模型
\begin_inset CommandInset citation
LatexCommand cite
key "LM-blog"
literal "false"

\end_inset

。
\end_layout

\begin_layout Standard
要让机器来评估文本是否符合人类的使用习惯，一种方式是通过语言学方面的研究，制定出人类语言的范式，比如：陈述句是由主谓宾构成的、定语修饰语需要加在名词前面等等。然
而，所有人类语言的共同特点就是字词组合具有非常大的灵活性，同一语义可以有多种表达方式，这给规则的制定带来了巨大的难题，因此我们不使用规则模型来完成我们的工作。
\end_layout

\begin_layout Standard
近年来，神经语言模型的发展为自然语言处理的文本分类等多个领域作出了巨大贡献，因此，我们可以考虑将本次实验的任务转化为在神经语言模型下的文本二分类问题，即利用句子
通顺为一类，不通顺为另一类来训练分类器，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.1"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示。
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/jim/Downloads/njuthesis/res/基础模型.PNG
	height 6cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
基础模型框架
\begin_inset CommandInset label
LatexCommand label
name "fig:1.1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Chapter
技术方案
\end_layout

\begin_layout Standard
在上一章我们讲到可以将本次工作转换为一个基于神经语言模型的句子分类问题，既然有了目标，那么现在的任务有两个：一是，特征提取及表示，由于我们使用的是中文，所以有基
于“字”的向量和基于“词”的向量可供选择；二是，神经语言模型的选择，目前用于文本分类的模型大致有CNN、RNN、(Bi)LSTM，还有最近很火热的Google团
队提出的BERT模型。
\end_layout

\begin_layout Standard
由于时间关系，在本次实验中，我仅选择了BiLSTM + Attention机制的模型和BERT模型分别进行了训练，并从中选择结果较优的一个作为最终结果。在具体讲
这两个模型之前，我先简单介绍一下特征提取和Word Embedding的相关内容。
\end_layout

\begin_layout Section
特征工程
\end_layout

\begin_layout Subsection
特征提取
\end_layout

\begin_layout Standard
由于本次实验给出的训练集存在较多的噪声，如句子中存在一些特殊字符和敏感词汇等等，我们首先需要做的就是将这些特殊词汇去除，然后再对句子进行基于字或词的拆分即可，其
中基于中文词的分词可采用jieba分词包
\begin_inset CommandInset citation
LatexCommand cite
key "jieba"
literal "false"

\end_inset

完成。
\end_layout

\begin_layout Subsection
Word Embedding
\end_layout

\begin_layout Standard
特征提取完成后的下一步就是将句子转换成向量表示，其中最简单的做法就是将词典中的每一个词都用一个one-hot向量来表示，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2.1"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
本文图片大部分来源于网络，在此声明
\end_layout

\end_inset

：
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/jim/Code/njuthesis/res/one-hot.jpg
	height 5cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
one-hot表示
\begin_inset CommandInset label
LatexCommand label
name "fig:2.1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
但是，这种表示方式存在两个明显的缺陷：
\end_layout

\begin_layout Itemize
one-hot向量的维度与词汇表的大小相等，容易造成维度灾难（几百万的词汇量就要用几百万维的向量表示）。
\end_layout

\begin_layout Itemize
one-hot的形式隐含了所有的单词都是独立正交的，然而在现实中，有许多词汇的意思是相近的，one-hot的表示无法体现出单词与单词之间的联系。例如“girl”
和“woman”都包含有女性的含义，但在one-hot的表示中这种关系就无法表达。
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
Word Embedding就解决了这两个问题，它的主要思想如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2.2"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示，即将单词高维的one-hot向量表示投影到一个低维的空间中，并试图通过探索词的上下文信息，来使得在这个低维的空间中表示中，具有相近词义的单词能够具有较近的
“距离”。具体细节可以参看文献
\begin_inset CommandInset citation
LatexCommand cite
key "word2vec2"
literal "false"

\end_inset

。
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/jim/Code/njuthesis/res/skip_gram_net_arch.png
	height 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Word Embedding示意图
\begin_inset CommandInset label
LatexCommand label
name "fig:2.2"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
关于Word Embedding的实现，现在已经有很多完备的开源工具可以使用，例如谷歌团队的Word2Vec
\begin_inset CommandInset citation
LatexCommand cite
key "word2vec2"
literal "false"

\end_inset

或GloVe
\begin_inset CommandInset citation
LatexCommand cite
key "glove"
literal "false"

\end_inset

，我们只需要提供语料并利用现有的任意工具训练得到词向量表示即可；或者可以使用以及训练好的开源词向量
\begin_inset CommandInset citation
LatexCommand cite
key "pre-word-vec"
literal "false"

\end_inset

，本次实验采用的是预训练的词向量。
\end_layout

\begin_layout Section
BiLSTM + Attention模型
\end_layout

\begin_layout Subsection
总体框架
\end_layout

\begin_layout Standard
关于BiLSTM + Attention模型来源于论文《Bidirectional Long Short-Term Memory Networks
 for Relation Classification》，就是是在BiLSTM的模型上加入Attention层，在Bi-LSTM中我们会用最后一个时序的输出向
量 作为特征向量，然后进行softmax分类。Attention是先计算每个时序的权重，然后将所有时序 的向量进行加权和作为特征向量，然后进行softmax分类
。在实验中，加上Attention确实对结果有所提升。其模型结构如下图：
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/jim/Downloads/njuthesis/res/BiLSTM.png
	height 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
BiLSTM+Attenstion示意图
\begin_inset CommandInset label
LatexCommand label
name "fig:2.3"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Attention
\begin_inset CommandInset citation
LatexCommand cite
key "Attention"
literal "false"

\end_inset

的作用
\end_layout

\begin_layout Standard
根据人类的阅读习惯进行思考，我们在阅读的时候，注意力通常不会平均分配在文本中的每个词。再回到上面的文本表示，如果直接将每个时刻的输出向量相加再平均，就等于认为每
个输入词对于文本表示的贡献是相等的，但实际情况往往不是这样，每个词汇应该有自己的权重。
\end_layout

\begin_layout Standard
所以在合并这些输出向量时，希望可以将注意力集中在那些对当前任务更重要的向量上。也就是给他们都分配一个权值，将所有的输出向量加权平均。假设输出向量为
\begin_inset Formula $h_{t}$
\end_inset

，权重为
\begin_inset Formula $\alpha_{t}$
\end_inset

则合并后的表示为：
\begin_inset Formula 
\[
s=\sum_{t}\alpha_{t}h_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
上文所说的为BiLSTM的每个输出向量分配不同权值，使得模型可以将注意力集中在重点几个词，降低其他无关词的作用的机制就是Attention机制。使用了Atten
tion机制可以使得文本表示的结果在当前的任务中更合理。
\end_layout

\begin_layout Section
BERT模型
\end_layout

\begin_layout Standard
最近由Google团队提出的BERT
\begin_inset CommandInset citation
LatexCommand cite
key "BERT"
literal "false"

\end_inset

模型在多个NLP任务中刷新了成绩，所以在本次实验中，我也来尝试一下将这个模型用于本次实验的任务。需要注意的是BERT一种fine-tuning的方法，它与fea
ture-based的方法有所不同。接下来将简单介绍一下这个模型，详细信息可以参看论文。
\end_layout

\begin_layout Subsection
模型架构
\end_layout

\begin_layout Standard
论文使用了两种模型
\begin_inset CommandInset citation
LatexCommand cite
key "BERT-detail"
literal "false"

\end_inset

：
\end_layout

\begin_layout Itemize
\begin_inset Formula $BERT_{BASE}$
\end_inset

: L=12, H=768, A=12, Total Parameters=110M
\end_layout

\begin_layout Itemize
\begin_inset Formula $BERT_{LARGE}$
\end_inset

: L=24, H=1024, A=16, Total Parameters=340M
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
这里L是layers层数(即Transformer blocks个数)，H是hidden vector size, A是self-attention的“头数”，
架构大体如图所示。
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/jim/Downloads/njuthesis/res/BERT.png
	height 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
BERT示意图
\begin_inset CommandInset label
LatexCommand label
name "fig:2.4"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
作为fine-tuning这一类的方法，作者提出了改进的方案，具体做法是：
\end_layout

\begin_layout Itemize
采取新的预训练的目标函数：随机mask输入中的一些tokens，然后在预训练中对它们进行预测
\end_layout

\begin_layout Itemize
增加句子级别的任务：建立句子间是否为上下承接关系的理解
\end_layout

\begin_layout Subsection
输入的表示
\end_layout

\begin_layout Standard
针对不同的任务，模型能够明确的表达一个句子。对于每一个token, 它的表征由其对应的token embedding, 段表征(segment
 embedding)和位置表征(position embedding)相加产生。如下图
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:3.1"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示
\begin_inset CommandInset citation
LatexCommand cite
key "BERT-detail"
literal "false"

\end_inset

：
\end_layout

\begin_layout Standard
\noindent
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/jim/Downloads/njuthesis/res/输入表示.png
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
BERT输入表示示意图
\begin_inset CommandInset label
LatexCommand label
name "fig:2.5"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
这种输入表示方式能够完美地契合分类任务的需求，也即我们本次实验的需求。
\end_layout

\begin_layout Chapter
结果分析与总结
\end_layout

\begin_layout Section
实验环境
\end_layout

\begin_layout Standard
实验环境及依赖如下：
\end_layout

\begin_layout Itemize
操作系统：macOS Mojave 10.14.1 
\end_layout

\begin_layout Itemize
处理器：3.6 GHz Intel Core i7-7700
\end_layout

\begin_layout Itemize
内存：32GB
\end_layout

\begin_layout Itemize
Python版本：3.6
\end_layout

\begin_layout Itemize
Tensorflow版本：BiLSTM模型为1.6.0（CPU版本）、BERT为1.11.0（CPU版本）
\end_layout

\begin_layout Itemize
jieba版本：0.39
\end_layout

\begin_layout Section
BiLSTM + Attention
\end_layout

\begin_layout Subsection
使用开源模型框架
\end_layout

\begin_layout Standard
本次实验采用了开源BiLSTM + Attention模型框架
\begin_inset CommandInset citation
LatexCommand cite
key "BiLSTM-code"
literal "false"

\end_inset

，并将其移植到我们的具体实验任务中来。
\end_layout

\begin_layout Standard
使用的超参数在随附代码的configure.py中可以找到，这里不在赘述。
\end_layout

\begin_layout Subsection
如何运行
\end_layout

\begin_layout Standard
训练：
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\$\ python\ train.py
\]

\end_inset


\end_layout

\begin_layout Standard
预测：
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\$\ python\ predict.py
\]

\end_inset


\end_layout

\begin_layout Subsection
性能分析
\end_layout

\begin_layout Standard
训练时间：如上配置训练500轮，总共耗时2天
\end_layout

\begin_layout Standard
F1-Score：在A榜为0.749左右；另外，由于最终采用了BERT的结果所以未保留BiLSTM训练好的模型，故没有它在B榜的分数（重新训练太耗费时间了）。
\end_layout

\begin_layout Section
BERT
\end_layout

\begin_layout Subsection
使用开源模型框架
\end_layout

\begin_layout Standard
本次实验采用了Google开源的BERT模型框架
\begin_inset CommandInset citation
LatexCommand cite
key "BERT-code"
literal "false"

\end_inset

，并将其移植到我们的具体实验任务中来，并使用与训练的中文模型chinese_L-12_H-768_A-12。
\end_layout

\begin_layout Standard
使用的超参数在随附代码的run.sh中可以找到，这里不在赘述。
\end_layout

\begin_layout Subsection
如何运行
\end_layout

\begin_layout Standard
参照run.sh中的命令运行。
\end_layout

\begin_layout Subsection
性能分析
\end_layout

\begin_layout Standard
训练时间：如上配置训练6轮，总共耗时7天
\end_layout

\begin_layout Standard
F1-Score：详见A榜和B榜的最终结果
\end_layout

\begin_layout Section
总结与展望
\end_layout

\begin_layout Subsection
总结
\end_layout

\begin_layout Standard
我对本次实验的总结如下：
\end_layout

\begin_layout Itemize
训练集的反例（不通顺）句子样本不足，导致直接使用softmax的反例置信度很低。
\end_layout

\begin_layout Itemize
测试集的数据分布与训练集相反，反例较多，导致模型不能很好地泛化。
\end_layout

\begin_layout Subsection
展望
\end_layout

\begin_layout Standard
我对本次实验的展望如下：
\end_layout

\begin_layout Itemize
因为训练集通顺的句子居多，所以在模型完全训练的情况下，它对通顺句子的判断的置信度应该会比高，所以，比较模型输出结果的分类概率，只有当模型以很高的概率（99%）判
断句子为通顺时才认为句子是通顺的，而不是简单地比较概率的大小关系来判断。
\end_layout

\begin_layout Itemize
希望能找到更多的反例来模拟真实数据分布。
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "LM-blog"
literal "false"

\end_inset

https://blog.csdn.net/u010899985/article/details/79177053
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "glove"
literal "false"

\end_inset

Pennington, Jeffrey, Richard Socher, and Christopher Manning.
 "Glove: Global vectors for word representation." Proceedings of the 2014
 conference on empirical methods in natural language processing (EMNLP).
 2014.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "jieba"
literal "false"

\end_inset

https://pypi.org/project/jieba/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "seq-tag"
literal "false"

\end_inset

Huang, Zhiheng, Wei Xu, and Kai Yu.
 "Bidirectional LSTM-CRF models for sequence tagging." arXiv preprint arXiv:1508.0
1991 (2015).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "tensorflow"
literal "false"

\end_inset

Abadi, Martín, et al.
 "Tensorflow: a system for large-scale machine learning." OSDI.
 Vol.
 16.
 2016.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "word2vec2"
literal "false"

\end_inset

Goldberg, Yoav, and Omer Levy.
 "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding
 method." arXiv preprint arXiv:1402.3722 (2014).
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "pre-word-vec"
literal "false"

\end_inset

https://github.com/Embedding/Chinese-Word-Vectors
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "Attention"
literal "false"

\end_inset

https://blog.csdn.net/thriving_fcl/article/details/73381217
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "BERT"
literal "false"

\end_inset

Devlin J , Chang M W , Lee K , et al.
 BERT: Pre-training of Deep Bidirectional Transformers for Language Understandin
g[J].
 2018.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "BERT-detail"
literal "false"

\end_inset

https://blog.csdn.net/triplemeng/article/details/83053419
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "BiLSTM-code"
literal "false"

\end_inset

https://github.com/SeoSangwoo/Attention-Based-BiLSTM-relation-extraction
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "BERT-code"
literal "false"

\end_inset

https://github.com/google-research/bert
\end_layout

\end_body
\end_document
